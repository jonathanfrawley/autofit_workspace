{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example: Result__\n",
        "\n",
        "In this example, we'll repeat the fit performed in `fit.py` of 1D data of a `Gaussian` + Exponential profile with 1D line\n",
        "data using the  non-linear  search emcee and inspect the *Result* object that is returned in detail.\n",
        "\n",
        "If you haven't already, you should checkout the files `example/model.py`,`example/analysis.py` and `example/fit.py` to\n",
        "see how the fit is performed by the code below. The first section of code below is simply repeating the commands in\n",
        "`example/fit.py`, so feel free to skip over it until you his the `Result`'s section.\n",
        "\n",
        "The attributes of the Result object are described in `overview/simple/result.py`. This example will not cover the\n",
        "attributes in full, and instead only focus on how the use of a more complex model changes the Result object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#%matplotlib inline\n",
        "\n",
        "import autofit as af\n",
        "import model as m\n",
        "import analysis as a\n",
        "\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "First, lets load data of a 1D `Gaussian` + 1D Exponential, by loading it from a .json file in the directory \n",
        "`autofit_workspace/dataset/`, which simulates the noisy data we fit (check it out to see how we simulate the \n",
        "data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1__exponential_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "Next, we create our model, which in this case corresponds to a `Gaussian` + Exponential. In model.py, you will have\n",
        "noted the `Gaussian` has 3 parameters (centre, intensity and sigma) and Exponential 3 parameters (centre, intensity and\n",
        "rate). These are the free parameters of our model that the `NonLinearSearch` fits for, meaning the non-linear\n",
        "parameter space has dimensionality = 6.\n",
        "\n",
        "In the simple example tutorial, we used a `Model` to create the model of the Gaussian. Models cannot be used to\n",
        "compose models from multiple model components and for this example we must instead use the Collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=m.Gaussian, exponential=m.Exponential)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkout `autofit_workspace/config/priors` - this config file defines the default priors of all our model\n",
        "components. However, we can overwrite priors before running the `NonLinearSearch` as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.gaussian.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.gaussian.intensity = af.UniformPrior(lower_limit=0.0, upper_limit=1e2)\n",
        "model.gaussian.sigma = af.UniformPrior(lower_limit=0.0, upper_limit=30.0)\n",
        "model.exponential.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.exponential.intensity = af.UniformPrior(lower_limit=0.0, upper_limit=1e2)\n",
        "model.exponential.rate = af.UniformPrior(lower_limit=0.0, upper_limit=10.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "We now set up our Analysis, using the class described in `analysis.py`. The analysis describes how given an instance\n",
        "of our model (a `Gaussian` + Exponential) we fit the data and return a log likelihood value. For this simple example,\n",
        "we only have to pass it the data and its noise-map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = a.Analysis(data=data, noise_map=noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "emcee = af.Emcee(\n",
        "    nwalkers=50,\n",
        "    nsteps=2000,\n",
        "    initializer=af.InitializerBall(lower_limit=0.49, upper_limit=0.51),\n",
        "    auto_correlations_settings=af.AutoCorrelationsSettings(\n",
        "        check_for_convergence=True,\n",
        "        check_size=100,\n",
        "        required_length=50,\n",
        "        change_threshold=0.01,\n",
        "    ),\n",
        "    number_of_cores=1,\n",
        ")\n",
        "\n",
        "result = emcee.fit(model=model, analysis=analysis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "__RESULT__\n",
        "\n",
        "Here, we'll look in detail at how the information contained in the result changes when we fit a more complex model. If\n",
        "you are unfamiliar with the result object, first read through `overview/simple/result.py`.\n",
        "\n",
        "First, we can note that the parameters list of lists now has 6 entries in the parameters column, given the \n",
        "dimensionality of the model has increased from N=3 to N=6.\n",
        "# %%\n",
        "'''\n",
        "samples = result.samples\n",
        "print(\"All Parameters:\")\n",
        "print(samples.parameters)\n",
        "print(\"Sample 10`s sixth parameter value (Exponential -> rate)\")\n",
        "print(samples.parameters[9][5], \"\\n\")\n",
        "\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "The vectors containing models have the same meaning as before, but they are also now of size 6 given the increase in\n",
        "model complexity.\n",
        "# %%\n",
        "'''\n",
        "print(\"Result and Error Vectors:\")\n",
        "print(samples.median_pdf_vector)\n",
        "print(samples.max_log_likelihood_vector)\n",
        "print(samples.max_log_posterior_vector)\n",
        "print(samples.vector_at_upper_sigma(sigma=3.0))\n",
        "print(samples.vector_at_lower_sigma(sigma=3.0))\n",
        "print(samples.error_vector_at_upper_sigma(sigma=3.0))\n",
        "print(samples.error_vector_at_lower_sigma(sigma=3.0), \"\\n\")\n",
        "\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "The parameter names and labels now contain 6 entries, including the Exponential class that was not included in the\n",
        "simple model example.\n",
        "# %%\n",
        "'''\n",
        "print(samples.model.model_component_and_parameter_names)\n",
        "print(samples.model.parameter_labels)\n",
        "print(\"\\n\")\n",
        "\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "When we return a result as an instance, it provides us with instances of the model using the Python classes used to\n",
        "compose it. Because our fit uses a Collection (as opposed to a `Model` in the simple example) the instance\n",
        "returned a dictionary named acoording to the names given to the Collection, which above were `gaussian` and\n",
        "`exponential`.\n",
        "# %%\n",
        "'''\n",
        "max_log_likelihood_instance = samples.max_log_likelihood_instance\n",
        "\n",
        "print(\"Max Log Likelihood `Gaussian` Instance:\")\n",
        "print(\"Centre = \", max_log_likelihood_instance.gaussian.centre)\n",
        "print(\"Intensity = \", max_log_likelihood_instance.gaussian.intensity)\n",
        "print(\"Sigma = \", max_log_likelihood_instance.gaussian.sigma, \"\\n\")\n",
        "print(\"Max Log Likelihood Exponential Instance:\")\n",
        "print(\"Centre = \", max_log_likelihood_instance.exponential.centre)\n",
        "print(\"Intensity = \", max_log_likelihood_instance.exponential.intensity)\n",
        "print(\"Sigma = \", max_log_likelihood_instance.exponential.rate, \"\\n\")\n",
        "\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "For our example problem of fitting a 1D `Gaussian` + Exponential profile, this makes it straight forward to plot \n",
        "the maximum likelihood model:\n",
        "# %%\n",
        "'''\n",
        "model_gaussian = max_log_likelihood_instance.gaussian.profile_from_xvalues(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "model_exponential = max_log_likelihood_instance.exponential.profile_from_xvalues(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "model_data = model_gaussian + model_exponential\n",
        "\n",
        "plt.plot(range(data.shape[0]), data)\n",
        "plt.plot(range(data.shape[0]), model_data)\n",
        "plt.plot(range(data.shape[0]), model_gaussian, \"--\")\n",
        "plt.plot(range(data.shape[0]), model_exponential, \"--\")\n",
        "plt.title(\"Illustrative model fit to 1D `Gaussian` + Exponential profile data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile intensity\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "All methods which give instances give us the same instance of a Collection:\n",
        "# %%\n",
        "'''\n",
        "print(samples.median_pdf_instance)\n",
        "print(samples.instance_at_upper_sigma)\n",
        "print(samples.instance_at_lower_sigma)\n",
        "print(samples.error_instance_at_upper_sigma)\n",
        "print(samples.error_instance_at_lower_sigma)\n",
        "print(samples.instance_from_sample_index(sample_index=500))\n",
        "\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "So that is that - adding model complexity doesn`t change a whole lot about the Result object, other than the switch\n",
        "to Collections meaning that our instances now have named entries.\n",
        "\n",
        "The take home point should be that when you name your model components, you should make sure to give them descriptive\n",
        "and information names that make the use of a result object clear and intuitive!\n",
        "# %%\n",
        "'''\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}