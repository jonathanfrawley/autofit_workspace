{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature: Database\n",
    "=================\n",
    "\n",
    "The default behaviour of **PyAutoFit** is for model-fitting results to be output to hard-disc in folders, which are\n",
    "straight forward to navigate and manually check. For small model-fitting tasks this is sufficient, however many users\n",
    "have a need to perform many model fits to very large datasets, making manual inspection of results time consuming.\n",
    "\n",
    "PyAutoFit's database feature outputs all model-fitting results as a\n",
    "sqlite3 (https://docs.python.org/3/library/sqlite3.html) relational database, such that all results\n",
    "can be efficiently loaded into a Jupyter notebook or Python script for inspection, analysis and interpretation. This\n",
    "database supports advanced querying, so that specific model-fits (e.g., which fit a certain model or dataset) can be\n",
    "loaded.\n",
    "\n",
    "This example extends our example of fitting a 1D `Gaussian` profile and fits 3 independent datasets each containing a\n",
    "1D Gaussian. The results will be written to a `.sqlite` database, which we will load to demonstrate the database.\n",
    "\n",
    "A full description of PyAutoFit's database tools is provided in the database chapter of the `HowToFit` lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:17.054072Z",
     "iopub.status.busy": "2021-04-15T15:02:17.053500Z",
     "iopub.status.idle": "2021-04-15T15:02:18.435936Z",
     "shell.execute_reply": "2021-04-15T15:02:18.435463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Jammy/Code/PyAuto/autofit_workspace\n",
      "Working Directory has been set to `/mnt/c/Users/Jammy/Code/PyAuto/autofit_workspace`\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pyprojroot import here\n",
    "workspace_path = str(here())\n",
    "%cd $workspace_path\n",
    "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
    "\n",
    "import autofit as af\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "\n",
    "import model as m\n",
    "import analysis as a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dataset Names__\n",
    "\n",
    "For each dataset we load it from hard-disc, set up its `Analysis` class and fit it with a non-linear search. \n",
    "\n",
    "The 3 datasets are in the `autofit_workspace/dataset/example_1d` folder.\n",
    "\n",
    "We want each results to be stored in the database with an entry specific to the dataset. We'll use the `Dataset`'s name \n",
    "string to do this, so lets create a list of the 3 dataset names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:18.438865Z",
     "iopub.status.busy": "2021-04-15T15:02:18.438528Z",
     "iopub.status.idle": "2021-04-15T15:02:18.440224Z",
     "shell.execute_reply": "2021-04-15T15:02:18.440479Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_names = [\"gaussian_x1_0\", \"gaussian_x1_1\", \"gaussian_x1_2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Info__\n",
    "\n",
    "Information about our model-fit that isn't part of the model-fit can be made accessible to the database, by passing \n",
    "an `info` dictionary. \n",
    "\n",
    "For example, below we write info on the dataset`s (hypothetical) data of observation and exposure time, which the\n",
    "database will be able to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:18.443122Z",
     "iopub.status.busy": "2021-04-15T15:02:18.442692Z",
     "iopub.status.idle": "2021-04-15T15:02:18.444839Z",
     "shell.execute_reply": "2021-04-15T15:02:18.445101Z"
    }
   },
   "outputs": [],
   "source": [
    "info = {\"date_of_observation\": \"01-02-18\", \"exposure_time\": 1000.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model__\n",
    "\n",
    "Next, we create our model, which again corresponds to a single `Gaussian` with manual priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:18.449057Z",
     "iopub.status.busy": "2021-04-15T15:02:18.448440Z",
     "iopub.status.idle": "2021-04-15T15:02:18.463028Z",
     "shell.execute_reply": "2021-04-15T15:02:18.462594Z"
    }
   },
   "outputs": [],
   "source": [
    "model = af.Collection(gaussian=m.Gaussian)\n",
    "\n",
    "model.gaussian.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
    "model.gaussian.intensity = af.LogUniformPrior(lower_limit=1e-2, upper_limit=1e2)\n",
    "model.gaussian.sigma = af.GaussianPrior(\n",
    "    mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=np.inf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Session__\n",
    "\n",
    "To output results directly to the database, we start a session, which includes the name of the database `.sqlite` file\n",
    "where results are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:18.466467Z",
     "iopub.status.busy": "2021-04-15T15:02:18.466055Z",
     "iopub.status.idle": "2021-04-15T15:02:18.476960Z",
     "shell.execute_reply": "2021-04-15T15:02:18.477371Z"
    }
   },
   "outputs": [],
   "source": [
    "session = af.db.open_database(\"database.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This for loop runs over every dataset, checkout the comments below for how we set up the database entry of each fit.\n",
    "\n",
    "Note how the `session` is passed to the `Emcee` search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:18.483403Z",
     "iopub.status.busy": "2021-04-15T15:02:18.482962Z",
     "iopub.status.idle": "2021-04-15T15:02:18.691904Z",
     "shell.execute_reply": "2021-04-15T15:02:18.691177Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: already completed, skipping non-linear search.\n",
      "INFO:root: already completed, skipping non-linear search.\n",
      "INFO:root: already completed, skipping non-linear search.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emcee has begun running. This Jupyter notebook cell with progress once Emcee has completed, this could take a few minutes!\n",
      "Emcee has begun running. This Jupyter notebook cell with progress once Emcee has completed, this could take a few minutes!\n",
      "Emcee has begun running. This Jupyter notebook cell with progress once Emcee has completed, this could take a few minutes!\n",
      "Emcee has finished run - you may now continue the notebook.\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in dataset_names:\n",
    "\n",
    "    \"\"\"\n",
    "    The code below loads the dataset and sets up the Analysis class.\n",
    "    \"\"\"\n",
    "    dataset_path = path.join(\"dataset\", \"example_1d\", dataset_name)\n",
    "\n",
    "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
    "    noise_map = af.util.numpy_array_from_json(\n",
    "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
    "    )\n",
    "\n",
    "    analysis = a.Analysis(data=data, noise_map=noise_map)\n",
    "\n",
    "    \"\"\"\n",
    "    In all examples so far, results have gone to the default output path, which was the `autofit_workspace/output` \n",
    "    folder and a folder named after the non linear search. In this example, we will repeat this process and then load\n",
    "    these results into the database and a `database.sqlite` file.\n",
    "    \n",
    "    However, results can be written directly to the `database.sqlite` file omitted hard-disc output entirely, which\n",
    "    can be important for performing large model-fitting tasks on high performance computing facilities where there\n",
    "    may be limits on the number of files allowed. The commented out code below shows how one would perform\n",
    "    direct output to the `.sqlite` file. \n",
    "    \n",
    "    [NOTE: direct writing to .sqlite not supported yet, so this fit currently outputs to hard-disc as per usual and\n",
    "    these outputs will be used to make the database.]\n",
    "    \"\"\"\n",
    "    emcee = af.Emcee(\n",
    "        path_prefix=path.join(\"features\", \"database\", dataset_name),\n",
    "        nwalkers=30,\n",
    "        nsteps=1000,\n",
    "        initializer=af.InitializerBall(lower_limit=0.49, upper_limit=0.51),\n",
    "        auto_correlations_settings=af.AutoCorrelationsSettings(\n",
    "            check_for_convergence=True,\n",
    "            check_size=100,\n",
    "            required_length=50,\n",
    "            change_threshold=0.01,\n",
    "        ),\n",
    "        number_of_cores=1,\n",
    "        session=session,\n",
    "    )\n",
    "\n",
    "    # INSERT CODE HERE.\n",
    "\n",
    "    print(\n",
    "        f\"Emcee has begun running. This Jupyter notebook cell with progress once Emcee has completed, this could take a \"\n",
    "        f\"few minutes!\"\n",
    "    )\n",
    "\n",
    "    result = emcee.fit(model=model, analysis=analysis, info=info)\n",
    "\n",
    "print(\"Emcee has finished run - you may now continue the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, note how the results are not contained in the `output` folder after each search completes. Instead, they are\n",
    "contained in the `database.sqlite` file, which we can load using the `Aggregator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:18.695772Z",
     "iopub.status.busy": "2021-04-15T15:02:18.695335Z",
     "iopub.status.idle": "2021-04-15T15:02:18.700834Z",
     "shell.execute_reply": "2021-04-15T15:02:18.701193Z"
    }
   },
   "outputs": [],
   "source": [
    "from autofit.database.aggregator import Aggregator\n",
    "\n",
    "agg = Aggregator.from_database(\"database.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the aggregator to inspect results, let me quickly cover Python generators. A generator is an object that \n",
    "iterates over a function when it is called. The aggregator creates all of the objects that it loads from the database \n",
    "as generators (as opposed to a list, or dictionary, or other Python type).\n",
    "\n",
    "Why? Because lists and dictionaries store every entry in memory simultaneously. If you fit many datasets, this will use \n",
    "a lot of memory and crash your laptop! On the other hand, a generator only stores the object in memory when it is used; \n",
    "Python is then free to overwrite it afterwards. Thus, your laptop won't crash!\n",
    "\n",
    "There are two things to bare in mind with generators:\n",
    "\n",
    " 1) A generator has no length and to determine how many entries it contains you first must turn it into a list.\n",
    "\n",
    " 2) Once we use a generator, we cannot use it again and need to remake it. For this reason, we typically avoid \n",
    " storing the generator as a variable and instead use the aggregator to create them on use.\n",
    "\n",
    "We can now create a `samples` generator of every fit. As we saw in the `result.py` example scripts, an instance of \n",
    "the `Samples` class acts as an interface to the results of the non-linear search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:18.704098Z",
     "iopub.status.busy": "2021-04-15T15:02:18.703672Z",
     "iopub.status.idle": "2021-04-15T15:02:18.820182Z",
     "shell.execute_reply": "2021-04-15T15:02:18.819717Z"
    }
   },
   "outputs": [],
   "source": [
    "samples_gen = agg.values(\"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we convert this generator to a list and it, the outputs are 3 different MCMCSamples instances. These correspond to \n",
    "the 3 model-fits performed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:18.824591Z",
     "iopub.status.busy": "2021-04-15T15:02:18.824110Z",
     "iopub.status.idle": "2021-04-15T15:02:18.826779Z",
     "shell.execute_reply": "2021-04-15T15:02:18.827131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emcee Samples:\n",
      "\n",
      "[<autofit.non_linear.mcmc.emcee.EmceeSamples object at 0x7ff1729c3ac0>]\n",
      "Total Samples Objects =  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Emcee Samples:\\n\")\n",
    "print(samples_gen)\n",
    "print(\"Total Samples Objects = \", len(list(samples_gen)), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Samples` class is described in the `result.py` example script. Using the `Aggregator` we can access all of the \n",
    "attributes described in that example, for example the value of every parameter.\n",
    "\n",
    "Refer to `result.py` for all the properties that are accessible via the `Aggregator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:19.052658Z",
     "iopub.status.busy": "2021-04-15T15:02:19.030385Z",
     "iopub.status.idle": "2021-04-15T15:02:19.055351Z",
     "shell.execute_reply": "2021-04-15T15:02:19.055022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters of the very first sample\n",
      "[50.97117756430036, 0.9661591358858549, 9.942457969408725]\n",
      "The tenth sample`s third parameter\n",
      "9.781870625177982 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for samples in agg.values(\"samples\"):\n",
    "    print(\"All parameters of the very first sample\")\n",
    "    print(samples.parameters[0])\n",
    "    print(\"The tenth sample`s third parameter\")\n",
    "    print(samples.parameters[9][2], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Querying__\n",
    "\n",
    "We can use the `Aggregator`'s to query the database and return only specific fits that we are interested in. We first \n",
    "do this, using the `info` object, whereby we can query any of its entries, for example the `dataset_name` string we \n",
    "input into the model-fit above. \n",
    "\n",
    "By querying using the string `gaussian_x1_1` the model-fit to only the second `Gaussian` dataset is returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:19.058004Z",
     "iopub.status.busy": "2021-04-15T15:02:19.057598Z",
     "iopub.status.idle": "2021-04-15T15:02:19.059569Z",
     "shell.execute_reply": "2021-04-15T15:02:19.059237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Missing\n",
    "\n",
    "# agg_query = agg.query(agg.directory.contains(\"gaussian_x1_1\"))\n",
    "# samples_gen = agg_query.values(\"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this list now has only 1 MCMCSamples corresponding to the second dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:19.062740Z",
     "iopub.status.busy": "2021-04-15T15:02:19.062327Z",
     "iopub.status.idle": "2021-04-15T15:02:19.064569Z",
     "shell.execute_reply": "2021-04-15T15:02:19.064944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<autofit.non_linear.mcmc.emcee.EmceeSamples object at 0x7ff1729c3ac0>]\n",
      "Total Samples Objects via dataset_name Query =  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(samples_gen)\n",
    "print(\"Total Samples Objects via dataset_name Query = \", len(list(samples_gen)), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query based on the model fitted. \n",
    "\n",
    "For example, we can load all results which fitted a `Gaussian` model-component, which in this simple example is all\n",
    "3 model-fits.\n",
    " \n",
    "The ability to query via the model is extremely powerful. It enables a user to perform many model-fits with many \n",
    "different model parameterizations to large datasets and efficiently load and inspect the results. \n",
    "\n",
    "[Note: the code `agg.gaussian` corresponds to the fact that in the `Collection` above, we named the model\n",
    "component `gaussian`. If this `Collection` had used a different name the code below would change \n",
    "correspondingly. Models with multiple model components (e.g., `gaussian` and `exponential`) are therefore also easily \n",
    "accessed via the database.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:19.068476Z",
     "iopub.status.busy": "2021-04-15T15:02:19.068055Z",
     "iopub.status.idle": "2021-04-15T15:02:19.201843Z",
     "shell.execute_reply": "2021-04-15T15:02:19.202375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples Objects via `Gaussian` model query =  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "gaussian = agg.gaussian\n",
    "agg_query = agg.query(gaussian == m.Gaussian)\n",
    "samples_gen = agg_query.values(\"samples\")\n",
    "print(\n",
    "    \"Total Samples Objects via `Gaussian` model query = \", len(list(samples_gen)), \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries using the results of model-fitting are also supported. Below, we query the database to find all fits where the \n",
    "inferred value of `sigma` for the `Gaussian` is less than 3.0 (which returns only the first of the three model-fits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:19.206906Z",
     "iopub.status.busy": "2021-04-15T15:02:19.206433Z",
     "iopub.status.idle": "2021-04-15T15:02:19.221732Z",
     "shell.execute_reply": "2021-04-15T15:02:19.220524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples Objects In Query `gaussian.sigma < 3.0` =  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "gaussian = agg.gaussian\n",
    "agg_query = agg.query(gaussian.sigma < 3.0)\n",
    "samples_gen = agg_query.values(\"samples\")\n",
    "print(\n",
    "    \"Total Samples Objects In Query `gaussian.sigma < 3.0` = \",\n",
    "    len(list(samples_gen)),\n",
    "    \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced queries can be constructed using logic, for example we below we combine the two queries above to find all\n",
    "results which fitted a `Gaussian` AND (using the & symbol) inferred a value of sigma less than 3.0. \n",
    "\n",
    "The OR logical clause is also supported via the symbol |."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T15:02:19.229619Z",
     "iopub.status.busy": "2021-04-15T15:02:19.229006Z",
     "iopub.status.idle": "2021-04-15T15:02:19.233666Z",
     "shell.execute_reply": "2021-04-15T15:02:19.233976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples Objects In Query `Gaussian & sigma < 3.0` =  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "gaussian = agg.gaussian\n",
    "agg_query = agg.query((gaussian == m.Gaussian) & (gaussian.sigma < 3.0))\n",
    "samples_gen = agg_query.values(\"samples\")\n",
    "print(\n",
    "    \"Total Samples Objects In Query `Gaussian & sigma < 3.0` = \",\n",
    "    len(list(samples_gen)),\n",
    "    \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API for querying is fairly self explanatory. Through the combination of info based queries, model based\n",
    "queries and result based queries a user has all the tools they need to fit extremely large datasets with many different\n",
    "models and load only the results they are interested in for inspection and analysis.\n",
    "\n",
    "The Database chapter of the **HowToFit** Jupyter notebooks give a full description of the database feature, including \n",
    "examples of advanced queries and how to load and plot the results of a model-fit in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
