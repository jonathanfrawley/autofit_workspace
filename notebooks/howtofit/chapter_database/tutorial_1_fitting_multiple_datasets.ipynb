{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial 1: Fitting Multiple Datasets\n",
    "=====================================\n",
    "\n",
    "The default behaviour of **PyAutoFit** is for model-fitting results to be output to hard-disc in folders, which are\n",
    "straight forward to navigate and manually check the model-fitting results and visualization. For small model-fitting\n",
    "tasks this is sufficient, however many users have a need to perform many model fits to very large datasets, making\n",
    "the manual inspection of results time consuming.\n",
    "\n",
    "PyAutoFit's database feature outputs all model-fitting results as a\n",
    "sqlite3 (https://docs.python.org/3/library/sqlite3.html) relational database, such that all results\n",
    "can be efficiently loaded into a Jupyter notebook or Python script for inspection, analysis and interpretation. This\n",
    "database supports advanced querying, so that specific model-fits (e.g., which fit a certain model or dataset) can be\n",
    "loaded.\n",
    "\n",
    "In this tutorial, we fit multiple dataset's with a `NonLinearSearch`, producing multiple sets of results on our\n",
    "hard-disc. In the following tutorials, we load these results using the database into our Jupyter notebook and\n",
    "interpret, inspect and plot the results.\n",
    "\n",
    "we'll fit 3 different dataset's, each with a single `Gaussian` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T19:03:39.368769Z",
     "iopub.status.busy": "2021-05-11T19:03:39.368257Z",
     "iopub.status.idle": "2021-05-11T19:03:40.981517Z",
     "shell.execute_reply": "2021-05-11T19:03:40.981820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Jammy/Code/PyAuto/autofit_workspace\n",
      "Working Directory has been set to `/mnt/c/Users/Jammy/Code/PyAuto/autofit_workspace`\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pyprojroot import here\n",
    "workspace_path = str(here())\n",
    "%cd $workspace_path\n",
    "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
    "\n",
    "import autofit as af\n",
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll reuse the `plot_line` and `Analysis` classes of the previous tutorial.\n",
    "\n",
    "Note that the `Analysis` class has a new method, `save_attributes_for_aggregator`. This method specifies which \n",
    "properties of the fit are output to hard-disc so that we can load them using the `Aggregator` in the next tutorials.\n",
    "\n",
    "In particular, note how we output the `data` and `noise_map`, these will be loaded in tutorial 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T19:03:40.991066Z",
     "iopub.status.busy": "2021-05-11T19:03:40.990658Z",
     "iopub.status.idle": "2021-05-11T19:03:40.992495Z",
     "shell.execute_reply": "2021-05-11T19:03:40.992765Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_line(\n",
    "    xvalues,\n",
    "    line,\n",
    "    title=None,\n",
    "    ylabel=None,\n",
    "    errors=None,\n",
    "    color=\"k\",\n",
    "    output_path=None,\n",
    "    output_filename=None,\n",
    "):\n",
    "    plt.errorbar(\n",
    "        x=xvalues, y=line, yerr=errors, color=color, ecolor=\"k\", elinewidth=1, capsize=2\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x value of profile\")\n",
    "    plt.ylabel(ylabel)\n",
    "    if not path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    plt.savefig(path.join(output_path, f\"{output_filename}.png\"))\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "class Analysis(af.Analysis):\n",
    "    def __init__(self, data, noise_map):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.noise_map = noise_map\n",
    "\n",
    "    def log_likelihood_function(self, instance):\n",
    "\n",
    "        model_data = self.model_data_from_instance(instance=instance)\n",
    "\n",
    "        residual_map = self.data - model_data\n",
    "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
    "        chi_squared = sum(chi_squared_map)\n",
    "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map ** 2.0))\n",
    "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def model_data_from_instance(self, instance):\n",
    "        \"\"\"\n",
    "        To create the summed profile of all individual profiles in an instance, we can use a dictionary comprehension\n",
    "        to iterate over all profiles in the instance.\n",
    "        \"\"\"\n",
    "        xvalues = np.arange(self.data.shape[0])\n",
    "\n",
    "        return sum(\n",
    "            [profile.profile_from_xvalues(xvalues=xvalues) for profile in instance]\n",
    "        )\n",
    "\n",
    "    def visualize(self, paths, instance, during_analysis):\n",
    "        \"\"\"\n",
    "        This method is identical to the previous tutorial, except it now uses the `model_data_from_instance` method\n",
    "        to create the profile.\n",
    "        \"\"\"\n",
    "        xvalues = np.arange(self.data.shape[0])\n",
    "\n",
    "        model_data = self.model_data_from_instance(instance=instance)\n",
    "\n",
    "        residual_map = self.data - model_data\n",
    "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
    "\n",
    "        \"\"\"\n",
    "        The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\n",
    "        \"\"\"\n",
    "        plot_line(\n",
    "            xvalues=xvalues,\n",
    "            line=self.data,\n",
    "            title=\"Data\",\n",
    "            ylabel=\"Data Values\",\n",
    "            color=\"k\",\n",
    "            output_path=paths.image_path,\n",
    "            output_filename=\"data\",\n",
    "        )\n",
    "\n",
    "        plot_line(\n",
    "            xvalues=xvalues,\n",
    "            line=model_data,\n",
    "            title=\"Model Data\",\n",
    "            ylabel=\"Model Data Values\",\n",
    "            color=\"k\",\n",
    "            output_path=paths.image_path,\n",
    "            output_filename=\"model_data\",\n",
    "        )\n",
    "\n",
    "        plot_line(\n",
    "            xvalues=xvalues,\n",
    "            line=residual_map,\n",
    "            title=\"Residual Map\",\n",
    "            ylabel=\"Residuals\",\n",
    "            color=\"k\",\n",
    "            output_path=paths.image_path,\n",
    "            output_filename=\"residual_map\",\n",
    "        )\n",
    "\n",
    "        plot_line(\n",
    "            xvalues=xvalues,\n",
    "            line=chi_squared_map,\n",
    "            title=\"Chi-Squared Map\",\n",
    "            ylabel=\"Chi-Squareds\",\n",
    "            color=\"k\",\n",
    "            output_path=paths.image_path,\n",
    "            output_filename=\"chi_squared_map\",\n",
    "        )\n",
    "\n",
    "    def save_attributes_for_aggregator(self, paths):\n",
    "        \"\"\"\n",
    "        Save files like the data and noise-map as pickle files so they can be loaded in the `Aggregator`\n",
    "        \"\"\"\n",
    "\n",
    "        # These functions save the objects we will later access using the aggregator. They are saved via the `pickle`\n",
    "        # module in Python, which serializes the data on to the hard-disk.\n",
    "\n",
    "        paths.save_object(\"data\", self.data)\n",
    "\n",
    "        paths.save_object(\"noise_map\", self.noise_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit the single `Gaussian` model used in chapter 1 of **HowToFit**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T19:03:40.995911Z",
     "iopub.status.busy": "2021-05-11T19:03:40.995556Z",
     "iopub.status.idle": "2021-05-11T19:03:41.011395Z",
     "shell.execute_reply": "2021-05-11T19:03:41.010908Z"
    }
   },
   "outputs": [],
   "source": [
    "import profiles as p\n",
    "\n",
    "model = af.Collection(gaussian=p.Gaussian)\n",
    "\n",
    "model.gaussian.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
    "model.gaussian.intensity = af.LogUniformPrior(lower_limit=1e-2, upper_limit=1e2)\n",
    "model.gaussian.sigma = af.GaussianPrior(\n",
    "    mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=np.inf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset we load it from hard-disc, set up its `Analysis` class and fit it with a non-linear search. \n",
    "\n",
    "The 3 datasets are in the `autofit_workspace/dataset/example_1d` folder.\n",
    "\n",
    "We want each results to be stored in the database with an entry specific to the dataset. We'll use the `Dataset`'s name \n",
    "string to do this, so lets create a list of the 3 dataset names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T19:03:41.014813Z",
     "iopub.status.busy": "2021-05-11T19:03:41.014462Z",
     "iopub.status.idle": "2021-05-11T19:03:41.016439Z",
     "shell.execute_reply": "2021-05-11T19:03:41.016695Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_names = [\"gaussian_x1_0\", \"gaussian_x1_1\", \"gaussian_x1_2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also attach information to the model-fit, by setting up an info dictionary. \n",
    "\n",
    "Information about our model-fit (e.g. the dataset) that isn't part of the model-fit is made accessible to the \n",
    "database. For example, below we write info on the dataset`s (hypothetical) data of observation and exposure time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T19:03:41.019223Z",
     "iopub.status.busy": "2021-05-11T19:03:41.018841Z",
     "iopub.status.idle": "2021-05-11T19:03:41.020324Z",
     "shell.execute_reply": "2021-05-11T19:03:41.020577Z"
    }
   },
   "outputs": [],
   "source": [
    "info = {\"date_of_observation\": \"01-02-18\", \"exposure_time\": 1000.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Session__\n",
    "\n",
    "To output results directly to the database, we start a session, which includes the name of the database `.sqlite` file\n",
    "where results are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T19:03:41.023273Z",
     "iopub.status.busy": "2021-05-11T19:03:41.022940Z",
     "iopub.status.idle": "2021-05-11T19:03:41.031876Z",
     "shell.execute_reply": "2021-05-11T19:03:41.032136Z"
    }
   },
   "outputs": [],
   "source": [
    "session = af.db.open_database(\"database_howtofit.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This for loop runs over every dataset, checkout the comments below for how we set up the path structure.\n",
    "\n",
    "Note how the `session` is passed to the `DynestyStatic` search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T19:03:41.035816Z",
     "iopub.status.busy": "2021-05-11T19:03:41.035455Z",
     "iopub.status.idle": "2021-05-11T19:03:41.282419Z",
     "shell.execute_reply": "2021-05-11T19:03:41.282698Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-11 20:03:41,051 - root - INFO -  already completed, skipping non-linear search.\n",
      "2021-05-11 20:03:41,136 - root - INFO -  already completed, skipping non-linear search.\n",
      "2021-05-11 20:03:41,195 - root - INFO -  already completed, skipping non-linear search.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emcee has begun running, checkout \n",
      "autofit_workspace/output/howtofit/database/gaussian_x1_0/tutorial_1_fitting_multiple_datasets folder for live \n",
      "output of the results. This Jupyter notebook cell with progress once Emcee has completed, this could take a \n",
      "few minutes!\n",
      "Emcee has begun running, checkout \n",
      "autofit_workspace/output/howtofit/database/gaussian_x1_1/tutorial_1_fitting_multiple_datasets folder for live \n",
      "output of the results. This Jupyter notebook cell with progress once Emcee has completed, this could take a \n",
      "few minutes!\n",
      "Emcee has begun running, checkout \n",
      "autofit_workspace/output/howtofit/database/gaussian_x1_2/tutorial_1_fitting_multiple_datasets folder for live \n",
      "output of the results. This Jupyter notebook cell with progress once Emcee has completed, this could take a \n",
      "few minutes!\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in dataset_names:\n",
    "\n",
    "    \"\"\"\n",
    "    The code below loads the dataset and sets up the Analysis class.\n",
    "    \"\"\"\n",
    "    dataset_path = path.join(\"dataset\", \"example_1d\", dataset_name)\n",
    "\n",
    "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
    "    noise_map = af.util.numpy_array_from_json(\n",
    "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
    "    )\n",
    "\n",
    "    analysis = Analysis(data=data, noise_map=noise_map)\n",
    "\n",
    "    \"\"\"\n",
    "    In all examples so far, results were wrriten to the `autofit_workspace/output` folder with a path and folder \n",
    "    named after the non-linear search.\n",
    "\n",
    "    In this example, results are written directly to the `database.sqlite` file after the model-fit is complete and \n",
    "    only stored in the output folder during the model-fit. This can be important for performing large model-fitting \n",
    "    tasks on high performance computing facilities where there may be limits on the number of files allowed, or there\n",
    "    are too many results to make navigating the output folder manually feasible.\n",
    "\n",
    "    The `unique_tag` uses the `dataset_name` name below to generate the unique identifier, which in other examples we \n",
    "    have seen is also generated depending on the search settings and model. In this example, all three model fits\n",
    "    use an identical search and model, so this `unique_tag` is key in ensuring 3 separate sets of results for each\n",
    "    model-fit are stored in the output folder and written to the .sqlite database. \n",
    "    \"\"\"\n",
    "    dynesty = af.DynestyStatic(\n",
    "        path_prefix=path.join(\"howtofit\", \"chapter_database\", dataset_name),\n",
    "        unique_tag=dataset_name,  # This makes the unique identifier use the dataset name\n",
    "        session=session,  # This instructs the search to write to the .sqlite database.\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Emcee has begun running, checkout \\n\"\n",
    "        f\"autofit_workspace/output/howtofit/database/{dataset_name}/tutorial_1_fitting_multiple_datasets folder for live \\n\"\n",
    "        f\"output of the results. This Jupyter notebook cell with progress once Emcee has completed, this could take a \\n\"\n",
    "        f\"few minutes!\"\n",
    "    )\n",
    "\n",
    "    dynesty.fit(model=model, analysis=analysis, info=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout the output folder, during the model-fits you should see three new sets of results corresponding to \n",
    "our 3 `Gaussian` datasets. If the model-fits are already complete, you will only see the .sqlite database file.\n",
    "\n",
    "This completes tutorial 1, which was less of a tutorial and more a quick exercise in getting the results of three \n",
    "model-fits onto our hard-disc to demonstrate **PyAutoFit**'s database feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
